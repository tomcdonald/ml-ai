{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Probability and Introduction to Jupyter Notebooks.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomcdonald/ml-ai/blob/master/notes/Probability_and_Introduction_to_Jupyter_Notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4hBHeGDIOGf",
        "colab_type": "text"
      },
      "source": [
        "# Probability & Introduction to Jupyter Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuhOU1QqJiVo",
        "colab_type": "text"
      },
      "source": [
        "Machine learning involves knowledge extraction from a dataset, with the aim of building a model which allows us to make predictions on unseen data. Many problems can be solved via this approach; examples include handwritten digit recognition, face recognition, stock market price prediction and recommendation systems.\n",
        "\n",
        "We can discuss this general approach in the context of the handwritten digit recognition example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnSiGbjWLLse",
        "colab_type": "text"
      },
      "source": [
        "* Each image can be transformed into a vector **x**, this is known as feature extraction.\n",
        "\n",
        "* An instance is made of the pair (**x**, y), where y is the image label.\n",
        "\n",
        "* The objective is to find a function f(**x**, **w**) which allows predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9CugzwyLwBY",
        "colab_type": "text"
      },
      "source": [
        "#### Basic Definitions\n",
        "\n",
        "* **Training set**: a set of N image and their labels, $(\\mathbf{x_1}, y_1), ... , (\\mathbf{x_N}, y_N)$\n",
        "\n",
        "* **Estimation or training phase**: process of getting the values of $\\mathbf{w}$ (sometimes known as 'weights') of the function $f(\\mathbf{x, w})$ which best fit the data.\n",
        "\n",
        "* **Generalisation**: ability to correctly predict the label of new images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slze-3JUOTPt",
        "colab_type": "text"
      },
      "source": [
        "#### Supervised & Unsupervised Learning\n",
        "\n",
        "* Supervised learning comes in two forms: classification where the variable $y$ is discrete, and regression where $y$ is continuous.\n",
        "\n",
        "* Unsupervised learning is used where we don't have access to any features (i.e. $y$), we only have access to the features, $\\mathbf{x_1, ..., x_N}$. This approach can be used to find groups of similar instances (*clustering*), to find a probability function for $\\mathbf{x}$ (*density estimation*), or to find a lower dimensionality representation of $\\mathbf{x}$ (*dimensionality reduction and feature selection*).\n",
        "\n",
        "* Other forms of learning do exist, such as semi-supervised learning, active learning and multi-task learning, but these are beyond the scope of this course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86nppi9yQj8K",
        "colab_type": "text"
      },
      "source": [
        "#### Linear Regression Example\n",
        "\n",
        "If two continuous variables $x$ and $y$ appear to follow a linear relationship, we may decide to model this relationship using a linear model:\n",
        "\n",
        "$y = w_1x + w_0$ where $w_0$ is the intercept and $w_1$ is the slope.\n",
        "\n",
        "We use an objective function to estimate the values of these two parameters which best fit the training dataset (i.e. give the lowest error); in this scenario, we use a least squares objective function:\n",
        "\n",
        "\\begin{equation}E(w_0, w_1) = \\sum_{\\forall i} (y_i - f(x_i))^2 = \\sum_{\\forall i} [y_i - (w_1x_i + w_0)]^2\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeCmP6q-U-07",
        "colab_type": "text"
      },
      "source": [
        "#### Probability Review\n",
        "\n",
        "If we look at two random variables $X$ and $Y$, which have outcomes denoted by $x$ and $y$, we can denote some key probability distributions as such:\n",
        "\n",
        "* **Joint** - the probability that $X=x$ and $Y=y$ is denoted $P(X=x, Y=y)$ or, for example:\n",
        "<br>\n",
        "<br>\n",
        "\\begin{equation}\\lim_{N\\to\\infty} \\frac{n_{X=3, Y=4}}{N}\\end{equation}\n",
        "<br>\n",
        "* **Marginal** - the probability that $X=x$ regardless of Y is denoted $P(X=x) or, for example:$\n",
        "<br>\n",
        "<br>\n",
        "\\begin{equation}\\lim_{N\\to\\infty} \\frac{n_{X=3}}{N}\\end{equation}\n",
        "<br>\n",
        "* **Conditional** - the probability that $X=x$ given that $Y=y$ is denoted $P(X=x | Y=y)$ or, for example:\n",
        "<br>\n",
        "<br>\n",
        "\\begin{equation}\\lim_{N\\to\\infty} \\frac{n_{X=3, Y=4}}{n_{Y=4}}\\end{equation}\n",
        "\n",
        "Note that whilst above, we've written out the probabilities fully (i.e. $P(X=x, Y=y)$), in practice we can typically abbreviate this to $P(x, y)$.\n",
        "\n",
        "Two obvious, but key notes regarding probability are that $P(x, y) = P(y, x)$, and also $\\sum_{x} P(x) = 1$; in other words, all probabilities must sum to 1, a concept known as normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJzQT3xAbhf1",
        "colab_type": "text"
      },
      "source": [
        "#### Sum Rule\n",
        "\n",
        "From the definitions of marginal probability ($P(y) = \\lim_{N\\to\\infty}\\frac{n_y}{N}$) and joint probability ($P(x,y) = \\lim_{N\\to\\infty}\\frac{n_{x,y}}{N}$), we can derive the sum rule of probability as follows:\n",
        "\n",
        "$n_y = \\sum_{x} n_{x,y}$, therefore: \\begin{equation}\\lim_{N\\to\\infty}\\frac{n_y}{N} = \\lim_{N\\to\\infty}\\sum_{x}\\frac{n_{x,y}}{N}\\end{equation}\n",
        "\n",
        "Or in a more succinct form:\n",
        "\n",
        "\\begin{equation}P(y)=\\sum_{x}P(x,y) \\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrRpT2L_fa0B",
        "colab_type": "text"
      },
      "source": [
        "#### Product Rule\n",
        "\n",
        "From the definitions of conditional probability ($P(x|y)=\\lim_{N\\to\\infty}\\frac{n_{x,y}}{n_y}$) and joint probability, we can derive the product rule of probability as follows:\n",
        "\n",
        "\\begin{equation}P(x,y)=\\lim_{N\\to\\infty}\\frac{n_{x,y}}{N}=\\lim_{N\\to\\infty}\\frac{n_{x,y}}{n_y}\\frac{n_y}{N} \\end{equation}\n",
        "\n",
        "Or in a more succinct form:\n",
        "\n",
        "\\begin{equation}P(x,y) = P(x|y)P(y) \\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQqPUHktgojv",
        "colab_type": "text"
      },
      "source": [
        "#### Bayes' Rule\n",
        "\n",
        "From the product rule:\n",
        "\n",
        "\\begin{equation} P(x,y)=P(y,x)=P(x|y)P(y) \\end{equation}\n",
        "\\begin{equation} P(y|x)P(x) = P(x|y)P(y) \\end{equation}\n",
        "\n",
        "which leads to Bayes' Rule, valid for $P(x)\\neq 0$\n",
        "\n",
        "\\begin{equation} P(y|x) = \\frac{P(x|y)P(y)}{P(x)} \\end{equation}"
      ]
    }
  ]
}